{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.optimizers import Momentum\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn import DNN\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TRAINING_DATA_PATH = 'dataset/Training/'\n",
    "VALIDATION_DATA_PATH = 'dataset/PublicTest/'\n",
    "IMAGE_INPUT_SIZE = 48 # correspondes with image size\n",
    "CHECKPOINT_PATH = \"checkpoints/chkpnt\"\n",
    "LOGS_PATH = \"logs\"\n",
    "MODEL_PATH = 'model/model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global\n",
    "dict_data = dict()\n",
    "dict_validation = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "keep_prob = 0.956\n",
    "learning_rate = 0.015\n",
    "optimizer_param = 0.97\n",
    "learning_rate_decay = 0.865\n",
    "decay_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training image data and transponse matrix\n",
    "dict_data['X'] = np.load(TRAINING_DATA_PATH + 'images.npy').reshape([-1, IMAGE_INPUT_SIZE, IMAGE_INPUT_SIZE, 1])\n",
    "dict_validation['X'] = np.load(VALIDATION_DATA_PATH + 'images.npy').reshape([-1, IMAGE_INPUT_SIZE, IMAGE_INPUT_SIZE, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[254]\n",
      "   [254]\n",
      "   [254]\n",
      "   ...\n",
      "   [ 16]\n",
      "   [  0]\n",
      "   [161]]\n",
      "\n",
      "  [[254]\n",
      "   [254]\n",
      "   [254]\n",
      "   ...\n",
      "   [ 17]\n",
      "   [  0]\n",
      "   [122]]\n",
      "\n",
      "  [[254]\n",
      "   [254]\n",
      "   [254]\n",
      "   ...\n",
      "   [ 26]\n",
      "   [  0]\n",
      "   [114]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 66]\n",
      "   [ 86]\n",
      "   [ 99]\n",
      "   ...\n",
      "   [255]\n",
      "   [252]\n",
      "   [254]]\n",
      "\n",
      "  [[ 84]\n",
      "   [ 92]\n",
      "   [ 92]\n",
      "   ...\n",
      "   [236]\n",
      "   [255]\n",
      "   [255]]\n",
      "\n",
      "  [[ 93]\n",
      "   [ 90]\n",
      "   [ 85]\n",
      "   ...\n",
      "   [ 42]\n",
      "   [129]\n",
      "   [180]]]\n",
      "\n",
      "\n",
      " [[[156]\n",
      "   [184]\n",
      "   [198]\n",
      "   ...\n",
      "   [157]\n",
      "   [154]\n",
      "   [150]]\n",
      "\n",
      "  [[146]\n",
      "   [182]\n",
      "   [199]\n",
      "   ...\n",
      "   [161]\n",
      "   [154]\n",
      "   [150]]\n",
      "\n",
      "  [[135]\n",
      "   [176]\n",
      "   [195]\n",
      "   ...\n",
      "   [165]\n",
      "   [161]\n",
      "   [155]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 28]\n",
      "   [ 25]\n",
      "   [ 21]\n",
      "   ...\n",
      "   [179]\n",
      "   [175]\n",
      "   [173]]\n",
      "\n",
      "  [[ 29]\n",
      "   [ 18]\n",
      "   [ 22]\n",
      "   ...\n",
      "   [177]\n",
      "   [172]\n",
      "   [169]]\n",
      "\n",
      "  [[ 21]\n",
      "   [ 14]\n",
      "   [ 23]\n",
      "   ...\n",
      "   [172]\n",
      "   [167]\n",
      "   [161]]]\n",
      "\n",
      "\n",
      " [[[ 69]\n",
      "   [118]\n",
      "   [ 61]\n",
      "   ...\n",
      "   [118]\n",
      "   [124]\n",
      "   [144]]\n",
      "\n",
      "  [[ 66]\n",
      "   [115]\n",
      "   [ 57]\n",
      "   ...\n",
      "   [129]\n",
      "   [123]\n",
      "   [131]]\n",
      "\n",
      "  [[ 64]\n",
      "   [116]\n",
      "   [ 61]\n",
      "   ...\n",
      "   [136]\n",
      "   [140]\n",
      "   [136]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[114]\n",
      "   [ 87]\n",
      "   [136]\n",
      "   ...\n",
      "   [ 98]\n",
      "   [ 84]\n",
      "   [ 86]]\n",
      "\n",
      "  [[114]\n",
      "   [ 85]\n",
      "   [140]\n",
      "   ...\n",
      "   [ 89]\n",
      "   [ 84]\n",
      "   [ 88]]\n",
      "\n",
      "  [[114]\n",
      "   [ 87]\n",
      "   [145]\n",
      "   ...\n",
      "   [ 88]\n",
      "   [ 87]\n",
      "   [ 90]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[255]\n",
      "   [255]\n",
      "   [255]\n",
      "   ...\n",
      "   [255]\n",
      "   [255]\n",
      "   [255]]\n",
      "\n",
      "  [[255]\n",
      "   [255]\n",
      "   [255]\n",
      "   ...\n",
      "   [255]\n",
      "   [255]\n",
      "   [255]]\n",
      "\n",
      "  [[255]\n",
      "   [255]\n",
      "   [255]\n",
      "   ...\n",
      "   [255]\n",
      "   [255]\n",
      "   [255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[232]\n",
      "   [231]\n",
      "   [218]\n",
      "   ...\n",
      "   [ 45]\n",
      "   [ 54]\n",
      "   [ 76]]\n",
      "\n",
      "  [[230]\n",
      "   [227]\n",
      "   [217]\n",
      "   ...\n",
      "   [ 55]\n",
      "   [ 54]\n",
      "   [ 50]]\n",
      "\n",
      "  [[228]\n",
      "   [220]\n",
      "   [207]\n",
      "   ...\n",
      "   [ 48]\n",
      "   [ 50]\n",
      "   [ 46]]]\n",
      "\n",
      "\n",
      " [[[ 33]\n",
      "   [ 25]\n",
      "   [ 31]\n",
      "   ...\n",
      "   [ 32]\n",
      "   [ 32]\n",
      "   [ 28]]\n",
      "\n",
      "  [[ 44]\n",
      "   [ 31]\n",
      "   [ 35]\n",
      "   ...\n",
      "   [ 32]\n",
      "   [ 31]\n",
      "   [ 25]]\n",
      "\n",
      "  [[ 23]\n",
      "   [ 35]\n",
      "   [ 37]\n",
      "   ...\n",
      "   [ 27]\n",
      "   [ 24]\n",
      "   [ 21]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  7]\n",
      "   [  5]\n",
      "   [  4]\n",
      "   ...\n",
      "   [  4]\n",
      "   [  4]\n",
      "   [  4]]\n",
      "\n",
      "  [[  4]\n",
      "   [  5]\n",
      "   [  4]\n",
      "   ...\n",
      "   [  4]\n",
      "   [  5]\n",
      "   [  4]]\n",
      "\n",
      "  [[  3]\n",
      "   [  4]\n",
      "   [  4]\n",
      "   ...\n",
      "   [  4]\n",
      "   [  5]\n",
      "   [  4]]]\n",
      "\n",
      "\n",
      " [[[ 61]\n",
      "   [ 63]\n",
      "   [ 59]\n",
      "   ...\n",
      "   [ 88]\n",
      "   [ 81]\n",
      "   [ 87]]\n",
      "\n",
      "  [[ 60]\n",
      "   [ 63]\n",
      "   [ 54]\n",
      "   ...\n",
      "   [ 85]\n",
      "   [ 83]\n",
      "   [ 84]]\n",
      "\n",
      "  [[ 58]\n",
      "   [ 65]\n",
      "   [ 53]\n",
      "   ...\n",
      "   [ 80]\n",
      "   [ 91]\n",
      "   [ 81]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 59]\n",
      "   [ 60]\n",
      "   [ 60]\n",
      "   ...\n",
      "   [124]\n",
      "   [120]\n",
      "   [197]]\n",
      "\n",
      "  [[ 58]\n",
      "   [ 60]\n",
      "   [ 59]\n",
      "   ...\n",
      "   [101]\n",
      "   [152]\n",
      "   [185]]\n",
      "\n",
      "  [[ 56]\n",
      "   [ 58]\n",
      "   [ 58]\n",
      "   ...\n",
      "   [113]\n",
      "   [165]\n",
      "   [180]]]]\n"
     ]
    }
   ],
   "source": [
    "print(dict_validation['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trainig landmarks data\n",
    "dict_data['X2'] = np.load(TRAINING_DATA_PATH + 'landmarks.npy')\n",
    "dict_validation['X2'] = np.load(VALIDATION_DATA_PATH + 'landmarks.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 7 17]\n",
      "  [ 7 21]\n",
      "  [ 7 26]\n",
      "  ...\n",
      "  [24 40]\n",
      "  [21 40]\n",
      "  [19 40]]\n",
      "\n",
      " [[ 3 21]\n",
      "  [ 5 26]\n",
      "  [ 7 31]\n",
      "  ...\n",
      "  [27 38]\n",
      "  [25 40]\n",
      "  [23 40]]\n",
      "\n",
      " [[ 8 21]\n",
      "  [ 8 26]\n",
      "  [ 9 31]\n",
      "  ...\n",
      "  [26 39]\n",
      "  [24 39]\n",
      "  [22 39]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 6 16]\n",
      "  [ 6 21]\n",
      "  [ 6 26]\n",
      "  ...\n",
      "  [25 37]\n",
      "  [23 37]\n",
      "  [22 37]]\n",
      "\n",
      " [[-1 28]\n",
      "  [ 1 33]\n",
      "  [ 3 38]\n",
      "  ...\n",
      "  [30 37]\n",
      "  [28 38]\n",
      "  [26 38]]\n",
      "\n",
      " [[ 5 19]\n",
      "  [ 6 24]\n",
      "  [ 7 29]\n",
      "  ...\n",
      "  [25 37]\n",
      "  [24 37]\n",
      "  [22 37]]]\n"
     ]
    }
   ],
   "source": [
    "print(dict_validation['X2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load labels\n",
    "dict_data['Y'] = np.load(TRAINING_DATA_PATH + 'labels.npy')\n",
    "dict_validation['Y'] = np.load(VALIDATION_DATA_PATH + 'labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(dict_validation['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model is divided in 2 seperate networks, which at the end get merged together.\n",
    "# 1. network: images 48x48\n",
    "# 2. network: 68 landmarks with each x,y coordinates\n",
    "def build_model():\n",
    "    # images network\n",
    "    network_images = input_data(shape=[None, IMAGE_INPUT_SIZE, IMAGE_INPUT_SIZE, 1], name='input_image')\n",
    "    # Convolutional Layer with filter size of 3 and 64 features\n",
    "    network_images = conv_2d(network_images, 64, 3, activation='relu')\n",
    "    # Normalization (remove negativ values)\n",
    "    network_images = batch_normalization(network_images)\n",
    "    # Pooling of size 3 and stride 2 => 48/2 = 24 => 24x24x64 (64 from conv layer) layer size\n",
    "    network_images = max_pool_2d(network_images, 3, strides=2)\n",
    "    # Convolutional Layer with filter size of 3 and 128 features\n",
    "    network_images = conv_2d(network_images, 128, 3, activation='relu')\n",
    "    # Normalization\n",
    "    network_images = batch_normalization(network_images)\n",
    "    # Pooling of size 3 and stride 2 => 24/2 = 24 => 12x12x128 (128 from conv layer) layer size\n",
    "    network_images = max_pool_2d(network_images, 3, strides=2)\n",
    "    # Convolutional Layer with filter size of 3 and 256 features\n",
    "    network_images = conv_2d(network_images, 256, 3, activation='relu')\n",
    "    # Normalization\n",
    "    network_images = batch_normalization(network_images)\n",
    "    # Pooling of size 3 and stride 2 => 12/2 = 6 => 6x6x256 (256 from conv layer) layer size\n",
    "    network_images = max_pool_2d(network_images, 3, strides=2)\n",
    "    # ignore some nodes for randomness\n",
    "    network_images = dropout(network_images, keep_prob=keep_prob)\n",
    "    # connect all nodes (6x6x256=9216 minus dropped) to 4096 output nodes\n",
    "    network_images = fully_connected(network_images, 4096, activation='relu')\n",
    "    # ignore some nodes for randomness\n",
    "    network_images = dropout(network_images, keep_prob=keep_prob)\n",
    "    # connect all nodes (4096 minus dropped) to 4096 output nodes\n",
    "    network_images = fully_connected(network_images, 1024, activation='relu')\n",
    "    # connect all nodes to 128 nodes to fit to landmarks for merge\n",
    "    network_images = fully_connected(network_images, 128, activation='relu')\n",
    "    \n",
    "    # landmarks network\n",
    "    network_landmarks = input_data(shape=[None, 68, 2], name='input_landmarks')\n",
    "    # connect all 68 landmarks to new 1024 output nodes\n",
    "    network_landmarks = fully_connected(network_landmarks, 1024, activation='relu')\n",
    "    # connect all 1024 landmarks to new 128 output nodes\n",
    "    network_landmarks = fully_connected(network_landmarks, 128, activation='relu')\n",
    "    \n",
    "    # merge the two networks together\n",
    "    network = merge([network_images, network_landmarks], 'concat', axis=1)\n",
    "    # finally connect them to the 7 emotions and use softmax to get values between 0..1 (for percentage)\n",
    "    network = fully_connected(network, 7, activation='softmax')\n",
    "    \n",
    "    # optimizer (for weight and bias)\n",
    "    optimizer = Momentum(learning_rate=learning_rate, momentum=optimizer_param, \n",
    "                    lr_decay=learning_rate_decay, decay_step=decay_step)\n",
    "    # regression function to get results\n",
    "    network = regression(network, optimizer=optimizer, loss='categorical_crossentropy', learning_rate=learning_rate, name='output')\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2924  | total loss: \u001b[1m\u001b[32m0.34200\u001b[0m\u001b[0m | time: 8.096s\n",
      "| Momentum | epoch: 013 | loss: 0.34200 - acc: 0.9099 -- iter: 28672/28709\n",
      "Training Step: 2925  | total loss: \u001b[1m\u001b[32m0.34067\u001b[0m\u001b[0m | time: 9.136s\n",
      "| Momentum | epoch: 013 | loss: 0.34067 - acc: 0.9056 | val_loss: 1.32749 - val_acc: 0.6063 -- iter: 28709/28709\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Start training graph\n",
    "with tf.Graph().as_default():\n",
    "    network = build_model()\n",
    "    model = DNN(network, tensorboard_dir=LOGS_PATH, \n",
    "                        tensorboard_verbose=0, checkpoint_path=CHECKPOINT_PATH,\n",
    "                        max_checkpoints=1)\n",
    "    \n",
    "    # start the training\n",
    "    model.fit([dict_data['X'], dict_data['X2']], dict_data['Y'],\n",
    "                                        validation_set=([dict_validation['X'], dict_validation['X2']], dict_validation['Y']),\n",
    "                                        snapshot_step=500,\n",
    "                                        show_metric=True,\n",
    "                                        batch_size=128,\n",
    "                                        n_epoch=13)\n",
    "    \n",
    "    # save the model\n",
    "    model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Jay\\Documents\\test\\model\\model.bin\n",
      "60.62970186723046%\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy validation data\n",
    "with tf.Graph().as_default():\n",
    "    network = build_model()\n",
    "    model = DNN(network, tensorboard_dir=LOGS_PATH, \n",
    "                        tensorboard_verbose=0, checkpoint_path=CHECKPOINT_PATH,\n",
    "                        max_checkpoints=1)\n",
    "    \n",
    "    model.load(MODEL_PATH)\n",
    "    accuracy = model.evaluate([dict_validation['X'], dict_validation['X2']], dict_validation['Y'])\n",
    "    print(str(accuracy[0]*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
